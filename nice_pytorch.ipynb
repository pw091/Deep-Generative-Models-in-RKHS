{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CUDA training on if detected:\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    CUDA = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    CUDA = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x, lo, hi):\n",
    "    \"\"\"Rescale a tensor to [lo,hi].\"\"\"\n",
    "    assert lo < hi, \"[rescale] lo={0} must be smaller than hi={1}\".format(lo, hi)\n",
    "    old_width = torch.max(x) - torch.min(x)\n",
    "    old_center = torch.min(x) + (old_width / 2.0)\n",
    "    new_width = float(hi - lo)\n",
    "    new_center = lo + (new_width / 2.0)\n",
    "    # shift everything back to zero:\n",
    "    x = x - old_center\n",
    "    # rescale to correct width:\n",
    "    x = x * (new_width / old_width)\n",
    "    # shift everything to the new center:\n",
    "    x = x + new_center\n",
    "    # return:\n",
    "    return x\n",
    "\n",
    "\n",
    "def zca_matrix(data_tensor):\n",
    "    \"\"\"\n",
    "    Helper function: compute ZCA whitening matrix across a dataset ~ (N, C, H, W).\n",
    "    \"\"\"\n",
    "    # 1. flatten dataset:\n",
    "    X = data_tensor.view(data_tensor.shape[0], -1)\n",
    "\n",
    "    # 2. zero-center the matrix:\n",
    "    X = rescale(X, -1.0, 1.0)\n",
    "\n",
    "    # 3. compute covariances:\n",
    "    cov = torch.t(X) @ X\n",
    "\n",
    "    # 4. compute ZCA(X) == U @ (diag(1/S)) @ torch.t(V) where U, S, V = SVD(cov):\n",
    "    U, S, V = torch.svd(cov)\n",
    "    return U @ torch.diag(torch.reciprocal(S)) @ torch.t(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_torchvision_data(dataset: str = \"mnist\"):\n",
    "    ### download training datasets:\n",
    "    if dataset == \"mnist\":\n",
    "        print(\"Downloading MNIST...\")\n",
    "        mnist = torchvision.datasets.MNIST(\n",
    "            root=\"./datasets/MNIST/torchvision\",\n",
    "            train=True,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True,\n",
    "        )\n",
    "    elif dataset == \"cifar10\":\n",
    "        print(\"Downloading CIFAR10...\")\n",
    "        cifar10 = torchvision.datasets.CIFAR10(\n",
    "            root=\"./datasets/CIFAR10\",\n",
    "            train=True,\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True,\n",
    "        )\n",
    "        ### save ZCA whitening matrices:\n",
    "        print(\"Computing CIFAR10 ZCA matrix...\")\n",
    "        torch.save(\n",
    "            zca_matrix(torch.cat([x for (x, _) in cifar10], dim=0)),\n",
    "            \"./datasets/CIFAR10/zca_matrix.pt\",\n",
    "        )\n",
    "    elif dataset == \"svhn\":\n",
    "        print(\"Downloading SVHN...\")\n",
    "        svhn = torchvision.datasets.SVHN(\n",
    "            root=\"./datasets/SVHN\",\n",
    "            split=\"train\",\n",
    "            transform=torchvision.transforms.ToTensor(),\n",
    "            download=True,\n",
    "        )\n",
    "        ### save ZCA whitening matrices:\n",
    "        print(\"Computing SVHN ZCA matrix...\")\n",
    "        torch.save(\n",
    "            zca_matrix(torch.cat([x for (x, _) in svhn], dim=0)),\n",
    "            \"./datasets/SVHN/zca_matrix.pt\",\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"Dataset {dataset} not implemented yet. Please choose from ['mnist', 'cifar10', 'svhn']\"\n",
    "        )\n",
    "    print(\"...All done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST...\n",
      "...All done.\n"
     ]
    }
   ],
   "source": [
    "download_torchvision_data(\"mnist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(train=True, batch_size=1, num_workers=0):\n",
    "    \"\"\"Rescale and preprocess MNIST dataset.\"\"\"\n",
    "    mnist_transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            # convert PIL image to tensor:\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            # flatten:\n",
    "            torchvision.transforms.Lambda(lambda x: x.view(-1)),\n",
    "            # add uniform noise:\n",
    "            torchvision.transforms.Lambda(\n",
    "                lambda x: (x + torch.rand_like(x).div_(256.0))\n",
    "            ),\n",
    "            # rescale to [0,1]:\n",
    "            torchvision.transforms.Lambda(lambda x: rescale(x, 0.0, 1.0)),\n",
    "        ]\n",
    "    )\n",
    "    return data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            root=\"./datasets/MNIST/torchvision\",\n",
    "            train=train,\n",
    "            transform=mnist_transform,\n",
    "            download=False,\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=CUDA,\n",
    "        drop_last=train,\n",
    "        num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    dataset = \"mnist\"\n",
    "    epochs = 1500\n",
    "    batch_size = 16\n",
    "    nlayers = 5\n",
    "    nhidden = 1000\n",
    "    prior = \"gaussian\"\n",
    "    lr = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.01\n",
    "    eps = 0.0001\n",
    "    lmbda = 0.0\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_even = lambda xs: xs[:, 0::2]\n",
    "_get_odd = lambda xs: xs[:, 1::2]\n",
    "\n",
    "\n",
    "def _interleave(first, second, order):\n",
    "    \"\"\"\n",
    "    Given 2 rank-2 tensors with same batch dimension, interleave their columns.\n",
    "\n",
    "    The tensors \"first\" and \"second\" are assumed to be of shape (B,M) and (B,N)\n",
    "    where M = N or N+1, repsectively.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    if order == \"even\":\n",
    "        for k in range(second.shape[1]):\n",
    "            cols.append(first[:, k])\n",
    "            cols.append(second[:, k])\n",
    "        if first.shape[1] > second.shape[1]:\n",
    "            cols.append(first[:, -1])\n",
    "    else:\n",
    "        for k in range(first.shape[1]):\n",
    "            cols.append(second[:, k])\n",
    "            cols.append(first[:, k])\n",
    "        if second.shape[1] > first.shape[1]:\n",
    "            cols.append(second[:, -1])\n",
    "    return torch.stack(cols, dim=1)\n",
    "\n",
    "\n",
    "class _BaseCouplingLayer(nn.Module):\n",
    "    def __init__(self, dim, partition, nonlinearity):\n",
    "        \"\"\"\n",
    "        Base coupling layer that handles the permutation of the inputs and wraps\n",
    "        an instance of torch.nn.Module.\n",
    "\n",
    "        Usage:\n",
    "        >> layer = AdditiveCouplingLayer(1000, 'even', nn.Sequential(...))\n",
    "\n",
    "        Args:\n",
    "        * dim: dimension of the inputs.\n",
    "        * partition: str, 'even' or 'odd'. If 'even', the even-valued columns are sent to\n",
    "        pass through the activation module.\n",
    "        * nonlinearity: an instance of torch.nn.Module.\n",
    "        \"\"\"\n",
    "        super(_BaseCouplingLayer, self).__init__()\n",
    "        # store input dimension of incoming values:\n",
    "        self.dim = dim\n",
    "        # store partition choice and make shorthands for 1st and second partitions:\n",
    "        assert partition in [\n",
    "            \"even\",\n",
    "            \"odd\",\n",
    "        ], \"[_BaseCouplingLayer] Partition type must be `even` or `odd`!\"\n",
    "        self.partition = partition\n",
    "        if partition == \"even\":\n",
    "            self._first = _get_even\n",
    "            self._second = _get_odd\n",
    "        else:\n",
    "            self._first = _get_odd\n",
    "            self._second = _get_even\n",
    "        # store nonlinear function module:\n",
    "        # (n.b. this can be a complex instance of torch.nn.Module, for ex. a deep ReLU network)\n",
    "        self.add_module(\"nonlinearity\", nonlinearity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Map an input through the partition and nonlinearity.\"\"\"\n",
    "        return _interleave(\n",
    "            self._first(x),\n",
    "            self.coupling_law(self._second(x), self.nonlinearity(self._first(x))),\n",
    "            self.partition,\n",
    "        )\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"Inverse mapping through the layer. Gradients should be turned off for this pass.\"\"\"\n",
    "        return _interleave(\n",
    "            self._first(y),\n",
    "            self.anticoupling_law(self._second(y), self.nonlinearity(self._first(y))),\n",
    "            self.partition,\n",
    "        )\n",
    "\n",
    "    def coupling_law(self, a, b):\n",
    "        # (a,b) --> g(a,b)\n",
    "        raise NotImplementedError(\n",
    "            \"[_BaseCouplingLayer] Don't call abstract base layer!\"\n",
    "        )\n",
    "\n",
    "    def anticoupling_law(self, a, b):\n",
    "        # (a,b) --> g^{-1}(a,b)\n",
    "        raise NotImplementedError(\n",
    "            \"[_BaseCouplingLayer] Don't call abstract base layer!\"\n",
    "        )\n",
    "\n",
    "\n",
    "class AdditiveCouplingLayer(_BaseCouplingLayer):\n",
    "    \"\"\"Layer with coupling law g(a;b) := a + b.\"\"\"\n",
    "\n",
    "    def coupling_law(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "    def anticoupling_law(self, a, b):\n",
    "        return a - b\n",
    "\n",
    "\n",
    "class MultiplicativeCouplingLayer(_BaseCouplingLayer):\n",
    "    \"\"\"Layer with coupling law g(a;b) := a .* b.\"\"\"\n",
    "\n",
    "    def coupling_law(self, a, b):\n",
    "        return torch.mul(a, b)\n",
    "\n",
    "    def anticoupling_law(self, a, b):\n",
    "        return torch.mul(a, torch.reciprocal(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_relu_network(latent_dim, hidden_dim, num_layers):\n",
    "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
    "    _modules = [nn.Linear(latent_dim, hidden_dim)]\n",
    "    for _ in range(num_layers):\n",
    "        _modules.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        _modules.append(nn.ReLU())\n",
    "        _modules.append(nn.BatchNorm1d(hidden_dim))\n",
    "    _modules.append(nn.Linear(hidden_dim, latent_dim))\n",
    "    return nn.Sequential(*_modules)\n",
    "\n",
    "\n",
    "class NICEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Replication of model from the paper:\n",
    "      \"Nonlinear Independent Components Estimation\",\n",
    "      Laurent Dinh, David Krueger, Yoshua Bengio (2014)\n",
    "      https://arxiv.org/abs/1410.8516\n",
    "\n",
    "    Contains the following components:\n",
    "    * four additive coupling layers with nonlinearity functions consisting of\n",
    "      five-layer RELUs\n",
    "    * a diagonal scaling matrix output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(NICEModel, self).__init__()\n",
    "        assert (\n",
    "            input_dim % 2 == 0\n",
    "        ), \"[NICEModel] only even input dimensions supported for now\"\n",
    "        assert num_layers > 2, \"[NICEModel] num_layers must be at least 3\"\n",
    "        self.input_dim = input_dim\n",
    "        half_dim = int(input_dim / 2)\n",
    "        self.layer1 = AdditiveCouplingLayer(\n",
    "            input_dim, \"odd\", _build_relu_network(half_dim, hidden_dim, num_layers)\n",
    "        )\n",
    "        self.layer2 = AdditiveCouplingLayer(\n",
    "            input_dim, \"even\", _build_relu_network(half_dim, hidden_dim, num_layers)\n",
    "        )\n",
    "        self.layer3 = AdditiveCouplingLayer(\n",
    "            input_dim, \"odd\", _build_relu_network(half_dim, hidden_dim, num_layers)\n",
    "        )\n",
    "        self.layer4 = AdditiveCouplingLayer(\n",
    "            input_dim, \"even\", _build_relu_network(half_dim, hidden_dim, num_layers)\n",
    "        )\n",
    "        self.scaling_diag = nn.Parameter(torch.ones(input_dim))\n",
    "\n",
    "        # randomly initialize weights:\n",
    "        for p in self.layer1.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                init.normal_(p, mean=0.0, std=0.001)\n",
    "        for p in self.layer2.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                init.normal_(p, mean=0.0, std=0.001)\n",
    "        for p in self.layer3.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                init.normal_(p, mean=0.0, std=0.001)\n",
    "        for p in self.layer4.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                init.normal_(p, mean=0.0, std=0.001)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"\n",
    "        Forward pass through all invertible coupling layers.\n",
    "\n",
    "        Args:\n",
    "        * xs: float tensor of shape (B,dim).\n",
    "\n",
    "        Returns:\n",
    "        * ys: float tensor of shape (B,dim).\n",
    "        \"\"\"\n",
    "        ys = self.layer1(xs)\n",
    "        ys = self.layer2(ys)\n",
    "        ys = self.layer3(ys)\n",
    "        ys = self.layer4(ys)\n",
    "        ys = torch.matmul(ys, torch.diag(torch.exp(self.scaling_diag)))\n",
    "        return ys\n",
    "\n",
    "    def inverse(self, ys):\n",
    "        \"\"\"Invert a set of draws from gaussians\"\"\"\n",
    "        with torch.no_grad():\n",
    "            xs = torch.matmul(\n",
    "                ys, torch.diag(torch.reciprocal(torch.exp(self.scaling_diag)))\n",
    "            )\n",
    "            xs = self.layer4.inverse(xs)\n",
    "            xs = self.layer3.inverse(xs)\n",
    "            xs = self.layer2.inverse(xs)\n",
    "            xs = self.layer1.inverse(xs)\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nice_loglkhd(h, diag):\n",
    "    \"\"\"\n",
    "    Definition of log-likelihood function with a Gaussian prior, as in the paper.\n",
    "\n",
    "    Args:\n",
    "    * h: float tensor of shape (N,D). First dimension is batch dim, second dim consists of components\n",
    "      of a factorized probability distribution.\n",
    "    * diag: scaling diagonal of shape (D,).\n",
    "\n",
    "    Returns:\n",
    "    * loss: torch float tensor of shape (N,).\n",
    "    \"\"\"\n",
    "    # \\sum^D_i s_{ii} - { (1/2) * \\sum^D_i  h_i**2) + (D/2) * log(2\\pi) }\n",
    "    return torch.sum(diag) - (\n",
    "        0.5 * torch.sum(torch.pow(h, 2), dim=1)\n",
    "        + h.size(1) * 0.5 * torch.log(torch.tensor(2 * np.pi))\n",
    "    )\n",
    "\n",
    "\n",
    "def logistic_nice_loglkhd(h, diag):\n",
    "    \"\"\"\n",
    "    Definition of log-likelihood function with a Logistic prior.\n",
    "\n",
    "    Same arguments/returns as gaussian_nice_loglkhd.\n",
    "    \"\"\"\n",
    "    # \\sum^D_i s_{ii} - { \\sum^D_i log(exp(h)+1) + torch.log(exp(-h)+1) }\n",
    "    return torch.sum(diag) - (\n",
    "        torch.sum(torch.log1p(torch.exp(h)) + torch.log1p(torch.exp(-h)), dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "# wrap above loss functions in Modules:\n",
    "class GaussianPriorNICELoss(nn.Module):\n",
    "    def __init__(self, size_average=True):\n",
    "        super(GaussianPriorNICELoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, fx, diag):\n",
    "        if self.size_average:\n",
    "            return torch.mean(-gaussian_nice_loglkhd(fx, diag))\n",
    "        else:\n",
    "            return torch.sum(-gaussian_nice_loglkhd(fx, diag))\n",
    "\n",
    "\n",
    "class LogisticPriorNICELoss(nn.Module):\n",
    "    def __init__(self, size_average=True):\n",
    "        super(LogisticPriorNICELoss, self).__init__()\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, fx, diag):\n",
    "        if self.size_average:\n",
    "            return torch.mean(-logistic_nice_loglkhd(fx, diag))\n",
    "        else:\n",
    "            return torch.sum(-logistic_nice_loglkhd(fx, diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.prior == \"logistic\":\n",
    "    nice_loss_fn = LogisticPriorNICELoss(size_average=True)\n",
    "else:\n",
    "    nice_loss_fn = GaussianPriorNICELoss(size_average=True)\n",
    "\n",
    "\n",
    "def loss_fn(fx):\n",
    "    \"\"\"Compute NICE loss w/r/t a prior and optional L1 regularization.\"\"\"\n",
    "    if args.lmbda == 0.0:\n",
    "        return nice_loss_fn(fx, model.scaling_diag)\n",
    "    else:\n",
    "        return nice_loss_fn(fx, model.scaling_diag) + args.lmbda * l1_norm(\n",
    "            model, include_bias=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"Construct a NICE model and train over a number of epochs.\"\"\"\n",
    "    # === choose which dataset to build:\n",
    "    if args.dataset == \"mnist\":\n",
    "        dataloader_fn = load_mnist\n",
    "        input_dim = 28 * 28\n",
    "    if args.dataset == \"svhn\":\n",
    "        dataloader_fn = load_svhn\n",
    "        input_dim = 32 * 32 * 3\n",
    "    if args.dataset == \"cifar10\":\n",
    "        dataloader_fn = load_cifar10\n",
    "        input_dim = 32 * 32 * 3\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"[train] dataset {} not supported\".format(args.dataset)\n",
    "        )\n",
    "\n",
    "    # === build model & optimizer:\n",
    "    model = NICEModel(input_dim, args.nhidden, args.nlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 28 * 28\n",
    "dataloader_fn = load_mnist\n",
    "dataloader = dataloader_fn(train=True, batch_size=args.batch_size)\n",
    "dataloader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NICEModel(input_dim, args.nhidden, args.nlayers)\n",
    "opt = optim.Adam(\n",
    "    model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2), eps=args.eps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3750 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n",
      "tensor([[0.0098, 0.0059, 0.0161,  ..., 0.0052, 0.0149, 0.0215],\n",
      "        [0.0103, 0.0086, 0.0142,  ..., 0.0054, 0.0146, 0.0142],\n",
      "        [0.0058, 0.0088, 0.0162,  ..., 0.0079, 0.0204, 0.0200],\n",
      "        ...,\n",
      "        [0.0062, 0.0089, 0.0084,  ..., 0.0063, 0.0169, 0.0232],\n",
      "        [0.0102, 0.0095, 0.0149,  ..., 0.0042, 0.0123, 0.0206],\n",
      "        [0.0055, 0.0110, 0.0090,  ..., 0.0043, 0.0112, 0.0152]],\n",
      "       grad_fn=<MmBackward0>) \n",
      "\n",
      "tensor(227.2925, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for inputs, _ in tqdm(dataloader):\n",
    "    opt.zero_grad()\n",
    "    fx = model(inputs)\n",
    "    print(fx.shape)\n",
    "    print(fx, \"\\n\")\n",
    "    loss = loss_fn(fx)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
