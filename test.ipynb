{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(\n",
    "    edgeitems=4,\n",
    "    linewidth=1000,\n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x),\n",
    "    suppress=True,\n",
    ")\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 100)\n",
    "torch.set_printoptions(edgeitems=2, linewidth=1000, precision=3, sci_mode=False)\n",
    "\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data shape: (60000, 784)\n",
      "Training data shape: (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/MNIST/mnist_train.csv\").drop([\"label\"], axis=1).values\n",
    "print(f\"Raw training data shape: {data.shape}\")\n",
    "data = data / 255\n",
    "# data = data.reshape(-1, 28, 28)\n",
    "print(f\"Training data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUNet(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=1000, num_layers=5):\n",
    "        super(ReLUNet, self).__init__()\n",
    "        # latend_dim = input_dim // 2 (split into either odds or evens)\n",
    "        # m: R^d -> R^{D-d}=R^{d} (ie: latent_dim -> input_dim - latent_dim = latent_dim)\n",
    "        modules = [nn.Linear(latent_dim, hidden_dim)]\n",
    "        for _ in range(num_layers):\n",
    "            modules.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            modules.append(nn.ReLU())\n",
    "        modules.append(nn.Linear(hidden_dim, latent_dim))\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class CouplingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive coupling layer for nice model:\n",
    "        g(a;b) = (a + b), where\n",
    "        a = x2, b = m(x1), and m is a recfified neural network (ReLUNet) from R^d -> R^{D-d}=R^{d}\n",
    "        Note: x1 and x2 are the odd and even parts of the input x, so for x in R^{D}, x1, x2 in R^{d}\n",
    "            d = D-d = D/2\n",
    "        Forward:\n",
    "            y1 = x1\n",
    "            y2 = g(x2;m(x1)) = x2 + m(x1)\n",
    "            y = (y1, y2)\n",
    "        Inverse:\n",
    "            x1 = y1\n",
    "            x2 = g^{-1}(y2;m(y1)) = y2 - m(y1)\n",
    "            x = (x1, x2)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, parity: str):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "\n",
    "        self.parity = parity\n",
    "        latent_dim = input_dim // 2\n",
    "\n",
    "        # Define NN layers for the transformation\n",
    "        self.m = ReLUNet(latent_dim, hidden_dim, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into \"odd\" and \"even\" parts\n",
    "        odd, even = x[:, 0::2], x[:, 1::2]\n",
    "        if self.parity == \"odd\":\n",
    "            x1, x2 = odd, even\n",
    "        else:\n",
    "            x1, x2 = even, odd\n",
    "\n",
    "        # Part 1 of the input is pass through an identity function (remains unchanged)\n",
    "        y1 = x1\n",
    "        # Apply the coupling transformation to the part 2 of the input\n",
    "        y2 = x2 + self.m(x1)\n",
    "\n",
    "        # Concatenate (or couple) the two parts back together\n",
    "        y = torch.cat([y1, y2], dim=1)\n",
    "        return y\n",
    "\n",
    "    def inverse(self, y):\n",
    "        # Split the output into two parts\n",
    "        odd, even = y[:, 0::2], y[:, 1::2]\n",
    "        if self.parity == \"odd\":\n",
    "            y1, y2 = odd, even\n",
    "        else:\n",
    "            y1, y2 = even, odd\n",
    "\n",
    "        # The first part of the output is unchanged\n",
    "        x1 = y1\n",
    "        # Apply the inverse transformation to the second part of the output\n",
    "        x2 = y2 - self.m(y1)\n",
    "\n",
    "        # Concatenate the two parts back together\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NICE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(NICE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Sequence of 4 alternativing parity coupling layers\n",
    "        self.couple1 = CouplingLayer(input_dim, hidden_dim, num_layers, parity=\"odd\")\n",
    "        self.couple2 = CouplingLayer(input_dim, hidden_dim, num_layers, parity=\"even\")\n",
    "        self.couple3 = CouplingLayer(input_dim, hidden_dim, num_layers, parity=\"odd\")\n",
    "        self.couple4 = CouplingLayer(input_dim, hidden_dim, num_layers, parity=\"even\")\n",
    "\n",
    "        \"\"\"\n",
    "        Create the scaling diagonal matrix (see Section 3.3 of the NICE paper)\n",
    "        Multiplies the ith output value by S_ii. Weights certain dim more than others.\n",
    "        Similar to eigenspectrum of PCA, exposing the variation present in each latent dimension \n",
    "        (larger S_ii means the less important dimension i is). More important dimensions of the \n",
    "        spctrum can be viewed as a manifold learned by the mdoel.\n",
    "        \"\"\"\n",
    "        self.scaling_diag = nn.Parameter(torch.ones(input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass is the encoding step of the NICE model.\n",
    "        \"\"\"\n",
    "        # Apply the coupling layers\n",
    "        y = self.couple1(x)\n",
    "        y = self.couple2(y)\n",
    "        y = self.couple3(y)\n",
    "        y = self.couple4(y)\n",
    "        # Apply the scaling layer\n",
    "        y = y @ torch.diag(torch.exp(self.scaling_diag))\n",
    "        return y\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"\n",
    "        Inverse pass is the decoding step of the NICE model.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Apply the inverse scaling layer\n",
    "            x = y @ torch.diag(torch.exp(-self.scaling_diag))\n",
    "            # Apply the inverse coupling layers\n",
    "            x = self.couple4.inverse(x)\n",
    "            x = self.couple3.inverse(x)\n",
    "            x = self.couple2.inverse(x)\n",
    "            x = self.couple1.inverse(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NICECriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of equation (3) above. Base class for Gaussian and Logistic criterion classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, average=True):\n",
    "        super(_NICECriterion, self).__init__()\n",
    "        self.average = average\n",
    "\n",
    "    def prior(self, h):\n",
    "        # Implement in child classes (4) and (5)\n",
    "        raise NotImplementedError(\"Must implement prior function in child class\")\n",
    "\n",
    "    def forward(self, h, s_diag):\n",
    "        # Implementation of (3). Identical for both Gaussian and Logistic.\n",
    "        # Don't take log of S_ii since it's already in log space, we take exp(S_ii) in forward pass.\n",
    "        log_p = torch.sum(self.prior(h), dim=1) + torch.sum(s_diag)\n",
    "        if self.average:\n",
    "            return torch.mean(log_p)\n",
    "        else:\n",
    "            return torch.sum(log_p)\n",
    "\n",
    "\n",
    "class GaussianNICECriterion(_NICECriterion):\n",
    "    \"\"\"\n",
    "    Implementation of (4) above. Gaussian prior based log-lokielihood critereon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, average=True):\n",
    "        super(GaussianNICECriterion, self).__init__()\n",
    "\n",
    "    def prior(self, h):\n",
    "        # Implementation of (4) above.\n",
    "        return -0.5 * (h**2 + torch.log(torch.tensor(2 * np.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 784])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epcohs = 1000\n",
    "batch_size = 16\n",
    "batch_np = data[:batch_size, :]\n",
    "batch = torch.from_numpy(batch_np).float()\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 1000\n",
    "num_layers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NICE(input_dim, hidden_dim, num_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "nice_loss_fn = GaussianNICECriterion()\n",
    "\n",
    "\n",
    "def loss_fn(y):\n",
    "    return nice_loss_fn(y, model.scaling_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "outputs = model(batch)\n",
    "loss = loss_fn(outputs)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.000, 0.000,  ..., 0.064, 0.099],\n",
       "        [0.000, 0.000,  ..., 0.066, 0.101],\n",
       "        ...,\n",
       "        [0.000, 0.000,  ..., 0.066, 0.098],\n",
       "        [0.000, 0.000,  ..., 0.061, 0.102]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-228.367, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loss.shape)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
